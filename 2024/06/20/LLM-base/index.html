<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="深情小小侯">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2024/06/20/llm-base/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="基本知识AttentionSelf-Attention的核心是用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息。 self-attention 的时间复杂度是在经过维度的“分割”之后，在多个低维空间，相比原有的高维空间，能降低特征学习的难度 ，复杂度相较单头并没有变化，主要还是transposes and reshapes 的操作，相当于把一个大矩阵相乘变成了多个小矩阵的相乘。">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM_base">
<meta property="og:url" content="http://example.com/2024/06/20/LLM-base/index.html">
<meta property="og:site_name" content="深情小小侯的博客">
<meta property="og:description" content="基本知识AttentionSelf-Attention的核心是用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息。 self-attention 的时间复杂度是在经过维度的“分割”之后，在多个低维空间，相比原有的高维空间，能降低特征学习的难度 ，复杂度相较单头并没有变化，主要还是transposes and reshapes 的操作，相当于把一个大矩阵相乘变成了多个小矩阵的相乘。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-06-20T03:38:11.000Z">
<meta property="article:modified_time" content="2024-10-19T03:01:20.537Z">
<meta property="article:author" content="深情小小侯">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            LLM_base -
        
        深情小小侯的博客
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/assets/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    window.config = {"hostname":"example.com","root":"/","language":"zh-CN","path":"search.xml"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":true,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"走得快走得慢不重要，走下去就是胜利","subtitle":{"text":[],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.5.0","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"文档":{"path":"/archives","icon":"fa-regular fa-archive"},"相册":{"icon":"fa-solid fa-image","submenus":{"假日出行":"/masonry","小乖":"/gallery"}}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"纸上得来终觉浅，绝知此事要躬行","links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    window.data = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                深情小小侯的博客
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        文档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-solid fa-image"></i>
                                        
                                        相册&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/masonry">假日出行
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/gallery">小乖
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                文档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-solid fa-image"></i>
                                
                                相册&nbsp;<i class="group-hover:rotate-180 transition-transform fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="drawer-navbar-item text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/masonry">假日出行</a>
                            </li>
                        
                            <li class="drawer-navbar-item text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/gallery">小乖</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular">LLM_base</h1>
            
            </div>
            
                    
        
        
            <div class="article-header flex flex-row gap-2 items-center">
                <div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
                    <img src="/images/redefine-avatar.svg">
                </div>
                <div class="info flex flex-col justify-between">
                    <div class="author flex items-center">
                        <span class="name text-default-text-color text-lg font-semibold">深情小小侯</span>
                        
                            <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv4</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-06-20 11:38:11</span>
        <span class="mobile">2024-06-20 11:38:11</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-10-19 11:01:20</span>
            <span class="mobile">2024-10-19 11:01:20</span>
            <span class="hover-info">更新</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Self-Attention的核心是用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息。</p>
<h3 id="self-attention-的时间复杂度是"><a href="#self-attention-的时间复杂度是" class="headerlink" title="self-attention 的时间复杂度是"></a>self-attention 的时间复杂度是</h3><p>在经过维度的“分割”之后，在多个低维空间，相比原有的高维空间，能降低特征学习的难度</p>
<p>，复杂度相较单头并没有变化，主要还是transposes and reshapes 的操作，相当于把一个大矩阵相乘变成了多个小矩阵的相乘。</p>
<h3 id="self-attention-在计算的过程中，如何对padding位做mask？"><a href="#self-attention-在计算的过程中，如何对padding位做mask？" class="headerlink" title="self-attention 在计算的过程中，如何对padding位做mask？"></a>self-attention 在计算的过程中，如何对padding位做mask？</h3><h3 id="1、LayerNorm、BatchNorm、RMSNorm"><a href="#1、LayerNorm、BatchNorm、RMSNorm" class="headerlink" title="1、LayerNorm、BatchNorm、RMSNorm"></a>1、LayerNorm、BatchNorm、<strong>RMSNorm</strong></h3><p>简单来说就是，虽然二者的时间复杂度一致，但是RMSNorm比起LayerNorm确实减少了减去均值以及加上bias的计算，这在目前大模型预训练的计算量下就能够体现出训练速度上的优势了，**并且RMSNorm在模型效果上的表现并不弱于LayerNorm(LN取得成功的原因可能是缩放不变性，而不是平移不变性)**，所以选择RMSNorm就很自然了, 要注意RMSNorm也是LaverNorm的一种，以上提到的LaverNorm指的是最常见的形式</p>
<table>
<thead>
<tr>
<th>正则化方法</th>
<th>工作原理</th>
<th>优点</th>
<th>缺点</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>LayerNorm</td>
<td>对每个样本在同一层的激活进行归一化</td>
<td>适用于变长序列、稳定训练</td>
<td>计算开销大、对大型模型效果一般在非常深的网络中，LayerNorm 的效果可能不如其他正则化方法，如 BatchNorm。</td>
<td>NLP模型、变长序列数据</td>
</tr>
<tr>
<td>BatchNorm</td>
<td>对每个批次的激活进行归一化</td>
<td>加速训练、提高性能</td>
<td>对批次大小敏感、训练推理不一致</td>
<td>计算机视觉模型、固定大小输入</td>
</tr>
<tr>
<td>RMSNorm</td>
<td>使用输入的均方根进行归一化</td>
<td>稳定性强、适合大模型、计算效率高</td>
<td>缺乏灵活性、适用场景有限</td>
<td>预训练模型、序列建模</td>
</tr>
</tbody></table>
<h3 id="2、说一下-Encoder-only、Encoder-Decoder、Decoder-only-的区别？"><a href="#2、说一下-Encoder-only、Encoder-Decoder、Decoder-only-的区别？" class="headerlink" title="2、说一下 Encoder-only、Encoder-Decoder、Decoder-only 的区别？"></a><strong>2、说一下 Encoder-only、Encoder-Decoder、Decoder-only 的区别？</strong></h3><p>在深度学习和自然语言处理（NLP）领域中，Encoder-Only、Decoder-Only以及Encoder-Decoder架构是三种不同类型的神经网络结构，它们各自设计用于不同的任务：</p>
<ol>
<li><p><strong>Encoder-Only架构</strong>：</p>
</li>
<li><ul>
<li><strong>定义与特点</strong>：这类模型仅包含编码器部分，主要用于从输入数据提取特征或表示。例如，在BERT (Bidirectional Encoder Representations from Transformers) 中，它是一个双向Transformer编码器，被训练来理解文本上下文信息，并输出一个固定长度的向量表示，该表示包含了原始输入序列的丰富语义信息。</li>
<li><strong>用途</strong>：主要用于预训练模型，如BERT、RoBERTa等，常用于各种下游任务的特征提取，比如分类、问答、命名实体识别等，但不直接用于生成新的序列。</li>
</ul>
</li>
<li><p><strong>Decoder-Only架构</strong>：</p>
</li>
<li><ul>
<li><strong>定义与特点</strong>：解码器仅架构专注于从某种内部状态或先前生成的内容生成新的序列，通常用于自回归式预测任务，其中每个时刻的输出都依赖于前面生成的所有内容。</li>
<li><strong>优点</strong>：强大的序列生成能力，能够按顺序逐个生成连续的元素（如单词、字符），适用于诸如文本生成、自动摘要、对话系统等生成性任务。典型的Decoder-Only模型包括GPT系列（如GPT-3）。</li>
</ul>
</li>
<li><p><strong>Encoder-Decoder架构</strong>：</p>
</li>
<li><ul>
<li><strong>定义与特点</strong>：这种架构由两个主要部分组成：编码器和解码器。编码器负责将输入序列转换为压缩的中间表示，解码器则基于这个中间表示生成目标输出序列。这种结构非常适合翻译、摘要生成、图像描述等任务，需要理解和重构输入信息后生成新序列的任务。</li>
<li><strong>工作原理</strong>：编码器对源序列进行处理并生成上下文向量，解码器根据此上下文向量逐步生成目标序列。例如，经典的Seq2Seq（Sequence-to-Sequence）模型和Transformer中的机器翻译模型就采用了这样的结构。</li>
</ul>
</li>
</ol>
<p>总结起来：</p>
<ul>
<li><strong>Encoder-Only</strong>用于理解输入并生成其抽象表示，不涉及序列生成。</li>
<li><strong>Decoder-Only</strong>专门用于根据之前的信息自动生成新序列，不接收外部输入。</li>
<li><strong>Encoder-Decoder</strong>结合了两者的功能，首先对输入进行编码，然后基于编码结果解码生成新序列。</li>
</ul>
<h3 id="3-模型输出的分布比较稀疏，怎么处理？"><a href="#3-模型输出的分布比较稀疏，怎么处理？" class="headerlink" title="3.模型输出的分布比较稀疏，怎么处理？"></a>3.<strong>模型输出的分布比较稀疏，怎么处理？</strong></h3><p>可以采用一些方法来处理模型输出的分布稀疏，例如使用softmax函数的温度参数调节来平滑输出分布，或者引入正则化技术，如Dropout，以减少模型对特定类别的过度依赖。</p>
<h3 id="4-kl散度的公式和kl散度与交叉熵的区别"><a href="#4-kl散度的公式和kl散度与交叉熵的区别" class="headerlink" title="4. kl散度的公式和kl散度与交叉熵的区别?"></a><strong>4. kl散度的公式和kl散度与交叉熵的区别?</strong></h3><h3 id="5-介绍一下-文本embedding方法"><a href="#5-介绍一下-文本embedding方法" class="headerlink" title="5. 介绍一下 文本embedding方法"></a>5. <strong>介绍一下 文本embedding方法</strong></h3><h3 id="6-说一下-transformer的模型架构和细节，如何并行"><a href="#6-说一下-transformer的模型架构和细节，如何并行" class="headerlink" title="6.说一下 transformer的模型架构和细节，如何并行"></a><strong>6.说一下 transformer的模型架构和细节</strong>，如何并行</h3><p>Transformer 的核心架构主要由两部分组成：</p>
<ul>
<li><strong>编码器（Encoder）</strong></li>
<li><strong>解码器（Decoder）</strong></li>
</ul>
<p>其中，每一部分又由多个<strong>自注意力层（Self-Attention Layers）</strong>和<strong>前馈神经网络层（Feed-Forward Neural Networks, FFN）</strong>组成。这些层内的计算能够很自然地实现并行化。</p>
<p><strong>如何并行</strong></p>
<ul>
<li><p>批处理</p>
</li>
<li><p>矩阵运算并行化</p>
<ul>
<li>张两操作：Transformer 中大量使用了矩阵和张量运算，这些操作本质上是高度并行化的。例如，矩阵乘法、张量的维度转换等操作可以在 GPU 上高效执行，因为 GPU 擅长处理大量的并行计算任务。</li>
<li>多头注意力的并行化：多头注意力中的每个头的计算是相互独立的，可以同时进行。并且，每个注意力头内部的矩阵运算也可以并行化执行。</li>
</ul>
</li>
<li><p>弄</p>
<p>除以根号d   scale factor Q 和 K 分别是查询（Query）和键（Key），维度均为 d。在计算 Q 和 K 的点积时，<strong>点积值的期望（Expectation）和方差（Variance）会随着 d 的增大而增大</strong>。可以有效地将点积值的方差控制在一个较为稳定的范围内，**使得它们不会因为维度 d 的增大而过大或过小</p>
</li>
</ul>
<h3 id="7-说一下大模型高校参数微调方式-p-tuning-v2？"><a href="#7-说一下大模型高校参数微调方式-p-tuning-v2？" class="headerlink" title="7.说一下大模型高校参数微调方式 p-tuning v2？"></a>7.<strong>说一下大模型高校参数微调方式 p-tuning v2？</strong></h3><p>P-Tuning V2在P-Tuning V1的基础上进行了下述改进：</p>
<ul>
<li>在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这与Prefix Tuning的做法相同。这样得到了更多可学习的参数，且更深层结构中的Prompt能给模型预测带来更直接的影响。</li>
<li>去掉了重参数化的编码器。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。</li>
<li>针对不同任务采用不同的提示长度。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。</li>
<li>可选的多任务学习。先在多任务的Prompt上进行预训练，然后再适配下游任务。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。</li>
</ul>
<h3 id="8-为什么现在的LLM都是Decoder-only的架构？"><a href="#8-为什么现在的LLM都是Decoder-only的架构？" class="headerlink" title="8 为什么现在的LLM都是Decoder only的架构？"></a><strong>8 为什么现在的LLM都是Decoder only的架构？</strong></h3><p>decoder only:<br>1、低秩问题，可能会削弱模型表达能力。<br>2、打破了位置不变性 -kv cache节省速度<br>3、flash attention 优化了框架结构、Scaling Law的支持。</p>
<ul>
<li><strong>自回归生成模型</strong>：即模型一次生成一个词，并基于已经生成的词继续生成下一个词。这种机制使得生成文本变得更加自然和连贯。<strong>连续性和一致性</strong>：自回归生成模型能够保证文本生成的连续性和一致性，因为每一步的输出都基于前面生成的内容。相比之下，Encoder-Decoder 架构则需要在生成每个词时处理整个上下文，这可能会导致不一致的生成结果。</li>
<li><strong>序列生成</strong>：Decoder-only 架构非常适合生成无限长的序列，因为每一步的生成只依赖于前面的输出，不需要重新计算整个上下文。这样可以有效地处理长文本生成任务。</li>
<li><strong>高效推理</strong>：由于不需要像 Encoder-Decoder 那样重新编码上下文，Decoder-only 架构在推理阶段更加高效，减少了计算资源的消耗。</li>
<li><strong>架构简单</strong>：Decoder-only 架构比 Encoder-Decoder 架构更简单，因为它只需要一个编码器（实际上是解码器）来处理输入并生成输出。这种简单性减少了模型的参数量和计算复杂度。</li>
<li><strong>训练效率</strong>：由于架构的简单性，Decoder-only 模型的训练过程更加高效。它只需要一次前向传播来生成整个序列，而不需要像 Encoder-Decoder 那样进行两次前向传播（一次编码，一次解码）。.</li>
</ul>
<p><strong>结论</strong></p>
<p>Decoder-only 架构之所以在现代大型语言模型中得到广泛应用，是因为它在模型设计、训练效率、生成性能和实际应用等方面具有诸多优势。这种架构能够高效地生成自然流畅的文本，适用于多种自然语言处理任务，并且在训练和部署方面表现出色。因此，越来越多的现代 LLMs 选择了 Decoder-only 的架构来满足不断增长的应用需求和性能要求。</p>
<h3 id="9-如何解决过拟合和欠拟合"><a href="#9-如何解决过拟合和欠拟合" class="headerlink" title="9.如何解决过拟合和欠拟合?"></a><strong>9.如何解决过拟合和欠拟合?</strong></h3><ul>
<li>过拟合（Overfitting）：</li>
<li><ul>
<li>增加数据量：通过增加训练数据来减少模型对特定数据的过度拟合。</li>
<li>简化模型：减少模型的复杂度，可以通过减少特征数量、降低多项式次数等方式。</li>
<li>正则化：引入正则化项，如L1或L2正则化，以惩罚模型复杂度。</li>
</ul>
</li>
<li>欠拟合（Underfitting）：</li>
<li><ul>
<li>增加特征：添加更多有意义的特征，提高模型的表达能力。</li>
<li>增加模型复杂度：选择更复杂的模型，如增加层数、节点数等。</li>
<li>减小正则化：减小正则化的程度，以允许模型更好地适应数据。.</li>
</ul>
</li>
</ul>
<p><strong>2 如何避免过&#x2F;欠拟合？</strong></p>
<ul>
<li>欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过增加网络复杂度或者在模型中增加特征，这些都是很好解决欠拟合的方法。</li>
<li>要想解决过拟合问题，就要显著减少测试误差而不过度增加训练误差，从而提高模型的泛化能力。我们可以使用正则化（Regularization）方法。</li>
</ul>
<p><strong>如何理解过拟合欠拟合</strong></p>
<ul>
<li>欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律</li>
<li>过拟合是指训练误差和测试误差之间的差距太大。换句换说，就是模型复杂度高于实际问题，模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集”死记硬背”（记住了不适用于测试集的训练集性质或特点），没有理解数据背后的规律，泛化能力差。</li>
</ul>
<h3 id="10-L1正则化和L2正则化的区别"><a href="#10-L1正则化和L2正则化的区别" class="headerlink" title="10. L1正则化和L2正则化的区别?"></a>10. L1正则化和L2正则化的区别?</h3><p>正则化是一种通过引入额外的约束或惩罚项，迫使模型的复杂度得到控制，从而减少过拟合的技术。具体来说，正则化是通过修改学习算法，使其在最小化损失函数的同时，也最小化模型复杂度。</p>
<ul>
<li><ul>
<li>L1正则化：</li>
<li><ul>
<li>增加的正则化项为权重向量的绝对值之和。</li>
<li>促使模型参数变得稀疏，即某些权重变为零，从而实现特征选择的效果。</li>
</ul>
</li>
<li>L2正则化：</li>
<li><ul>
<li>增加的正则化项为权重向量的平方和。</li>
<li>通过减小权重的同时保持它们都非零，对权重进行平滑调整。</li>
</ul>
</li>
<li>区别：</li>
<li><ul>
<li>L1正则化倾向于产生稀疏权重，对于特征选择有利；</li>
<li>L2正则化则更倾向于在所有特征上产生较小但非零的权重。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="11-介绍一下-常见大模型结构（llama2，glm）？"><a href="#11-介绍一下-常见大模型结构（llama2，glm）？" class="headerlink" title="11.介绍一下 常见大模型结构（llama2，glm）？"></a>11.介绍一下 常见大模型结构（llama2，glm）？</h3><ul>
<li>BART (bi Encoder+casual Decoder，类bert的方法预训练)</li>
<li>T5 (Encoder+Decoder，text2text预训练)</li>
<li>GPT(Decoder主打zero-shot)</li>
<li>GLM (mask的输入部分是双向注意力，在生成预测的是单向注意力)<ul>
<li><strong>双向注意力和单向生成</strong>：GLM 采用了双向注意力机制来处理输入部分，使得输入的每个词都可以同时关注前后的词；在生成阶段则使用单向注意力，即每一步的生成仅依赖于之前的内容。</li>
<li><strong>混合注意力机制</strong>：这种混合的注意力机制使 GLM 能够既有效利用上下文信息，又保证生成文本的连贯性和合理性。</li>
</ul>
</li>
</ul>
<p><strong>结论</strong></p>
<table>
<thead>
<tr>
<th>模型</th>
<th>架构</th>
<th>预训练方法</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LLaMA2</strong></td>
<td>Decoder-only</td>
<td>自回归语言建模</td>
<td>文本生成、文本理解</td>
</tr>
<tr>
<td><strong>GLM</strong></td>
<td>双向注意力（输入），单向生成（输出）</td>
<td>双向掩码语言建模、自回归生成</td>
<td>问答系统、文本生成</td>
</tr>
<tr>
<td><strong>BART</strong></td>
<td>Encoder-Decoder</td>
<td>降噪自动编码、多任务训练</td>
<td>文本翻译、文本修复</td>
</tr>
<tr>
<td><strong>T5</strong></td>
<td>Encoder-Decoder</td>
<td>无监督文本到文本、多任务学习</td>
<td>文本生成、文本翻译</td>
</tr>
<tr>
<td><strong>GPT</strong></td>
<td>Decoder-only</td>
<td>自回归语言建模</td>
<td>文本生成、零样本学习</td>
</tr>
</tbody></table>
<h3 id="12-Sigmoid、ReLU-和-GELU"><a href="#12-Sigmoid、ReLU-和-GELU" class="headerlink" title="12 Sigmoid、ReLU 和 GELU"></a>12 <strong>Sigmoid</strong>、<strong>ReLU</strong> 和 <strong>GELU</strong></h3><hr>
<h2 id="推理量化加速"><a href="#推理量化加速" class="headerlink" title="推理量化加速"></a>推理量化加速</h2><p><strong>3 介绍一下 ptuning，lora tune？</strong></p>
<hr>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h3 id="1-gpt源码past-key-value是干啥的？"><a href="#1-gpt源码past-key-value是干啥的？" class="headerlink" title="1.  gpt源码past_key_value是干啥的？"></a><strong>1.  gpt源码past_key_value是干啥的？</strong></h3><p>在GPT（Generative Pre-trained Transformer）中，past_key_value是用于存储先前层的注意力权重的结构。在进行推理时，过去的注意力权重可以被重复使用，避免重复计算，提高效率。</p>
<h3 id="2-pt-onebyone-每一层怎么输入输出？"><a href="#2-pt-onebyone-每一层怎么输入输出？" class="headerlink" title="2. pt onebyone 每一层怎么输入输出？"></a><strong>2. pt onebyone 每一层怎么输入输出？</strong></h3><p>在GPT One-by-One中，每一层的输入是上一层的输出。具体而言，输入是一个序列的嵌入表示（通常是词嵌入），并通过自注意力机制和前馈神经网络进行处理，得到输出序列的表示。</p>
<h3 id="3-chatgpt的reward-model怎么来的，三阶段？"><a href="#3-chatgpt的reward-model怎么来的，三阶段？" class="headerlink" title="3 chatgpt的reward model怎么来的，三阶段？"></a><strong>3 chatgpt的reward model怎么来的，三阶段？</strong></h3><p>ChatGPT模型的训练过程中，确实涉及到了一个基于人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）的三阶段过程。以下是这个过程的一个概述：</p>
<ol>
<li><p><strong>预训练（Pre-training）阶段</strong>：</p>
</li>
<li><ul>
<li>在这个阶段，GPT模型通过无监督学习的方式在大规模文本数据集上进行训练。该模型的目标是预测下一个词语给定前面的词语序列，从而学习语言模型的基本结构和模式。</li>
</ul>
</li>
<li><p><strong>奖励模型训练（Reward Model Training）阶段</strong>：</p>
</li>
<li><ul>
<li>预训练后的模型会被用于生成大量针对各种提示的回答。</li>
<li>这些生成的回答会由人工标注员进行评估，并给出好坏或满意度得分，形成一个带有质量评分的数据集。</li>
<li>基于这些人工标注的数据，训练一个奖励模型（Reward Model），该模型可以预测对于任何给定的输入和输出对，人类用户可能给予多大的满意程度分数。</li>
<li>通过这种方式，奖励模型能够理解并量化哪些类型的回答更符合人类期望的标准。</li>
</ul>
</li>
<li><p><strong>强化学习微调（Fine-tuning with Reinforcement Learning）阶段</strong>：</p>
</li>
<li><ul>
<li>使用训练好的奖励模型作为指导信号，将预训练模型与强化学习算法结合，对模型进行微调（fine-tuning）。</li>
<li>模型现在以强化学习的方式进一步训练，目标是在生成响应时最大化来自奖励模型的预期奖励，也就是得到更高的满意度分数。</li>
<li>通过迭代优化，ChatGPT模型逐渐学会根据上下文生成更加准确、有用且合乎伦理道德的回答。</li>
</ul>
</li>
</ol>
<p>最终，经过这三阶段训练流程，ChatGPT不仅具备了强大的语言生成能力，还能够更好地理解和适应人类对话的需求，提供更为高质量的人工智能交互体验。</p>
<hr>
<h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p>BERT（Bidirectional Encoder Representations from Transformers）是一种深度双向预训练语言表示模型，由Google AI提出，其核心是基于Transformer架构，旨在通过大规模无标注文本数据预训练模型，然后在特定任务上进行微调，以获得优秀的表现。以下是BERT模型的结构和关键细节：</p>
<h3 id="结构和组成部分"><a href="#结构和组成部分" class="headerlink" title="结构和组成部分"></a>结构和组成部分</h3><ol>
<li><strong>Transformer 架构基础</strong>：<ul>
<li>BERT模型基于Transformer的编码器部分，Transformer是一种结构由多头自注意力机制和前馈神经网络组成的序列到序列模型。</li>
</ul>
</li>
<li><strong>输入表示</strong>：<ul>
<li>BERT的输入是文本序列的词嵌入（Word Embeddings），包括词向量（Word Embeddings）和位置编码（Positional Encodings）的结合。这些词向量可以是通过预训练过程中学习得到的或者是直接使用预训练模型中的固定词向量。</li>
</ul>
</li>
<li><strong>层次结构</strong>：<ul>
<li>BERT模型通常包含多个Transformer编码器层，这些层堆叠在一起形成深层网络结构。BERT-base包含12个层，而BERT-large包含24个层。这些层都是完全相同的，只是层数不同。</li>
</ul>
</li>
<li><strong>Transformer 编码器层</strong>：<ul>
<li>每个Transformer编码器层由两个子层组成：<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：用于捕捉输入序列中各个位置之间的依赖关系，允许输入中的单词相互交互。</li>
<li><strong>全连接前馈神经网络（Feedforward Neural Network）</strong>：在每个位置上分别应用，用于处理每个位置的信息。</li>
</ul>
</li>
</ul>
</li>
<li><strong>残差连接和层归一化</strong>：<ul>
<li>每个子层后面都跟随着残差连接（Residual Connection）和层归一化（Layer Normalization），这些技术有助于加速训练和提高模型的收敛速度。</li>
</ul>
</li>
<li><strong>预训练任务</strong>：<ul>
<li>BERT通过两个主要的预训练任务来学习通用的语言表示：<ul>
<li><strong>掩码语言建模（Masked Language Model, MLM）</strong>：在输入序列中随机掩盖一些词，然后预测这些位置上原本的词。</li>
<li><strong>下一句预测（Next Sentence Prediction, NSP）</strong>：模型预测两个句子是否在语义上相邻。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="1-在BERT中，token分3种情况做mask，分别的作用是什么？"><a href="#1-在BERT中，token分3种情况做mask，分别的作用是什么？" class="headerlink" title="1.在BERT中，token分3种情况做mask，分别的作用是什么？"></a><strong>1.在BERT中，token分3种情况做mask，分别的作用是什么？</strong></h3><p>15%token做mask；其中80%用[MASK]替换，10%用random token替换，10%不变。其实这个就是典型的Denosing Autoencoder的思路，那些被Mask掉的单词就是在输入侧加入的所谓噪音</p>
<p>这么做的主要原因是：</p>
<ol>
<li>在后续finetune任务中语句中并不会出现 [MASK] 标记；</li>
<li>预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。</li>
</ol>
<h3 id="2-BERT训练时使用的学习率-warm-up-策略是怎样的？为什么要这么做？"><a href="#2-BERT训练时使用的学习率-warm-up-策略是怎样的？为什么要这么做？" class="headerlink" title="2.BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？"></a>2.<strong>BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？</strong></h3><p>warmup 需要在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay。</p>
<p>这是因为，刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；在第一轮训练的时候，每个数据点对模型来说都是新的，模型会很快地进行数据分布修正，如果这时候学习率就很大，极有可能导致开始的时候就对该数据“过拟合”，后面要通过多轮训练才能拉回来，浪费时间。当训练了一段时间（比如两轮、三轮）后，模型已经对每个数据点看过几遍了，或者说对当前的batch而言有了一些正确的先验，较大的学习率就不那么容易会使模型学偏，所以可以适当调大学习率。这个过程就可以看做是warmup。那么为什么之后还要decay呢？当模型训到一定阶段后（比如十个epoch），模型的分布就已经比较固定了，或者说能学到的新东西就比较少了。如果还沿用较大的学习率，就会破坏这种稳定性，用我们通常的话说，就是已经接近loss的local optimal了，为了靠近这个point，我们就要慢慢来。</p>
<h3 id="3-BERT预训练任务"><a href="#3-BERT预训练任务" class="headerlink" title="3. BERT预训练任务"></a>3. BERT预训练任务</h3><p>ERT（Enhanced Representation through Transformation）是一种在大模型中常见的预训练任务设置。你提到的两部分任务分别是 MLM（Masked Language Model）和 NSP（Next Sentence Prediction），它们是 BERT（Bidirectional Encoder Representations from Transformers）预训练任务的核心部分。下面详细解释这两个任务：</p>
<ul>
<li><h4 id="MLM（Masked-Language-Model）"><a href="#MLM（Masked-Language-Model）" class="headerlink" title="MLM（Masked Language Model）"></a>MLM（Masked Language Model）</h4></li>
</ul>
<p>MLM 是一种单词级别的分类任务，旨在让模型学会从上下文中预测被掩盖的单词。</p>
<p> 具体步骤：</p>
<ol>
<li><strong>掩盖单词</strong>： 在给定的句子中，随机掩盖15%的单词，用 <code>[MASK]</code> 进行替换。例如，句子“我喜欢吃苹果”可能被修改为“我喜欢[MASK]苹果”。</li>
<li><strong>模型预测</strong>： 模型根据上下文来预测被掩盖的单词是什么。它需要根据句子中的其他单词来猜测 <code>[MASK]</code> 处的单词。例如，在上面的例子中，模型应该预测出 <code>[MASK]</code> 是“吃”。</li>
<li><strong>目标</strong>： 模型的目标是最小化预测的误差，使得预测的单词尽可能接近实际的被掩盖单词。</li>
</ol>
<p> 重要性：</p>
<p>MLM 任务使模型能够学习到每个单词的上下文关系，从而能够更好地理解语言的语义和句法结构。</p>
<ul>
<li><h4 id="NSP（Next-Sentence-Prediction）"><a href="#NSP（Next-Sentence-Prediction）" class="headerlink" title="NSP（Next Sentence Prediction）"></a><strong>NSP（Next Sentence Prediction）</strong></h4></li>
</ul>
<p>NSP 是一种句子级别的分类任务，旨在让模型学会判断两个句子之间是否存在某种关系，例如，第二个句子是否是第一个句子的下一句。</p>
<p>具体步骤：</p>
<ol>
<li><strong>生成句对</strong>： 给定一对句子 <code>(A, B)</code>，其中 B 可能是 A 的下一句，也可能不是。模型需要判断 B 是否为 A 的下一句。</li>
<li><strong>模型预测</strong>： 模型通过比较两个句子的上下文特征，判断 B 是否是 A 的下一句。比如，给定句子对“我喜欢读书”，“我正在读一本关于历史的书”，模型应当判断这两句话是相关的。</li>
<li><strong>目标</strong>： 模型的目标是通过最小化分类误差来提高句子关系预测的准确性。</li>
</ol>
<p> 重要性：</p>
<p>NSP 任务使模型能够理解句子之间的连贯性和逻辑关系，从而提升了模型在处理自然语言理解任务时的效果。</p>
<p><strong>为什么 ERT 任务重要？</strong></p>
<p>ERT 任务，尤其是 BERT 中的 MLM 和 NSP 任务，能够显著提高模型对语言的理解能力。通过 MLM 任务，模型可以深入理解每个单词在不同上下文中的意义，而通过 NSP 任务，模型能够学习到句子之间的逻辑关系。这种预训练方式使得模型在各种下游任务（如问答、文本分类、情感分析等）中表现得更加优越。</p>
<p> 应用场景：</p>
<ul>
<li><strong>文本理解</strong>：如问答系统、阅读理解。</li>
<li><strong>信息检索</strong>：如搜索引擎中的查询理解和文档相关性评估。</li>
<li><strong>机器翻译</strong>：提升翻译质量。</li>
<li><strong>情感分析</strong>：准确识别文本的情感倾向。</li>
</ul>
<p>这些预训练任务的设计，使得大语言模型在处理不同类型的自然语言任务时，能够更好地捕捉和利用语言中的信息，提高了模型的泛化能力和任务表现。</p>
<hr>
<h2 id="RAG-相关面试题"><a href="#RAG-相关面试题" class="headerlink" title="RAG 相关面试题"></a><strong>RAG 相关面试题</strong></h2><h3 id="1-聊一下RAG项目总体思路？"><a href="#1-聊一下RAG项目总体思路？" class="headerlink" title="1. 聊一下RAG项目总体思路？"></a>1. 聊一下RAG项目总体思路？</h3><h3 id="2-使用外挂知识库主要是为了解决什么问题？"><a href="#2-使用外挂知识库主要是为了解决什么问题？" class="headerlink" title="2. 使用外挂知识库主要是为了解决什么问题？"></a>2. 使用外挂知识库主要是为了解决什么问题？</h3><h3 id="3-如何评价RAG项目的效果好坏，即指标是什么？"><a href="#3-如何评价RAG项目的效果好坏，即指标是什么？" class="headerlink" title="3. 如何评价RAG项目的效果好坏，即指标是什么？"></a>3. 如何评价RAG项目的效果好坏，即指标是什么？</h3><h3 id="4-在做RAG项目过程中遇到哪些问题？怎么解决的？"><a href="#4-在做RAG项目过程中遇到哪些问题？怎么解决的？" class="headerlink" title="4. 在做RAG项目过程中遇到哪些问题？怎么解决的？"></a>4. 在做RAG项目过程中遇到哪些问题？怎么解决的？</h3><h3 id="5-RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？"><a href="#5-RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？" class="headerlink" title="5. RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？"></a>5. RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？</h3><h3 id="6-数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？"><a href="#6-数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？" class="headerlink" title="6. 数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？"></a>6. 数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？</h3><h3 id="7-模型底座是什么，这些不同底座什么区别，什么规模？"><a href="#7-模型底座是什么，这些不同底座什么区别，什么规模？" class="headerlink" title="7. 模型底座是什么，这些不同底座什么区别，什么规模？"></a>7. 模型底座是什么，这些不同底座什么区别，什么规模？</h3><h3 id="8-使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？"><a href="#8-使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？" class="headerlink" title="8. 使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？"></a>8. 使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？</h3><h3 id="9-模型推理是怎么做的，有没有cot，tot等等，还是单轮？"><a href="#9-模型推理是怎么做的，有没有cot，tot等等，还是单轮？" class="headerlink" title="9. 模型推理是怎么做的，有没有cot，tot等等，还是单轮？"></a>9. 模型推理是怎么做的，有没有cot，tot等等，还是单轮？</h3><h3 id="10-大模型可控性如何实现，怎么保证可控性？"><a href="#10-大模型可控性如何实现，怎么保证可控性？" class="headerlink" title="10. 大模型可控性如何实现，怎么保证可控性？"></a>10. 大模型可控性如何实现，怎么保证可控性？</h3><h3 id="11-模型部署的平台，推理效率怎么样，如何提升推理效率？"><a href="#11-模型部署的平台，推理效率怎么样，如何提升推理效率？" class="headerlink" title="11. 模型部署的平台，推理效率怎么样，如何提升推理效率？"></a>11. 模型部署的平台，推理效率怎么样，如何提升推理效率？</h3><h3 id="4-RAG-解决了哪些问题"><a href="#4-RAG-解决了哪些问题" class="headerlink" title=".4 RAG 解决了哪些问题?"></a><strong>.4 RAG 解决了哪些问题?</strong></h3><ol>
<li>长尾知识: 对于一些相对通用和大众的知识，LLM 通常能生成比较准确的结果，而对于一些长尾知识，LLM 生成的回复通常并不可靠。</li>
<li>私有数据: ChatGPT 这类通用的 LLM 预训练阶段利用的大部分都是公开的数据，不包含私有数据，因此对于一些私有领域知识是欠缺的。</li>
<li>数据新鲜度: LLM 通过从预训练数据中学到的这部分信息就很容易过时。</li>
<li>来源验证和可解释性: 通常情况下，LLM 生成的输出不会给出其来源，比较难解释为什么会这么生成。而通过给 LLM 提供外部数据源，让其基于检索出的相关信息进行生成，就在生成的结果和信息来源之间建立了关联，因此生成的结果就可以追溯参考来源，可解释性和可控性就大大增强。</li>
</ol>
<h3 id="13-给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？"><a href="#13-给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？" class="headerlink" title="13. 给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？"></a>13. 给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？</h3><hr>
<h2 id="Agent-相关面试题"><a href="#Agent-相关面试题" class="headerlink" title="Agent 相关面试题"></a><strong>Agent 相关面试题</strong></h2><ol>
<li>了解 Ai Agent 么？</li>
<li>知道 Multi-Agent 么？</li>
<li>Multi-Agent 如何 实现多代理协作的？</li>
<li>Multi-Agent 如何 实现多代理竞争的？</li>
<li>你了解哪些 开源 Agent？有部署过么？效果怎么样？</li>
</ol>
<hr>
<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><h3 id="1-lora的原理"><a href="#1-lora的原理" class="headerlink" title="1. lora的原理"></a>1. <strong>lora的原理</strong></h3><p>LoRA的基本原理是冻结预训练的模型参数，然后在Transfomer的每一层中加入一个可训练的旁路矩阵（低秩可分离矩阵），接着将旁路输出与初始路径输出相加输入到网络当中，并只训练这些新增的旁路矩阵参数。其中，低秩可分离矩阵由两个矩阵组成，第一个矩阵负责降维，第二个矩阵负责升维，中间层维度为r，从而来模拟本征秩（intrinsic rank），这两个低秩矩阵能够大幅度减小参数量。</p>
<ul>
<li>在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的intrinsic rank。</li>
<li>训练的时候固定 PLM 的参数，只训练降维矩阵 A 与升维矩阵 B 。而模型的输入输出维度不变，输出时将 BA 与 PLM 的参数叠加。</li>
<li>用随机高斯分布初始化 A ，用 0 矩阵初始化 B ，保证训练的开始此旁路矩阵依然是 0 矩阵。</li>
</ul>
<h3 id="2-lora的矩阵怎么初始化？为什么要初始化为全0？"><a href="#2-lora的矩阵怎么初始化？为什么要初始化为全0？" class="headerlink" title="2.lora的矩阵怎么初始化？为什么要初始化为全0？"></a>2.<strong>lora的矩阵怎么初始化？为什么要初始化为全0？</strong></h3><p>初始化时，矩阵 A 随机高斯初始化，矩阵 B 初始化为0。之所以要这样初始化的原因是，在初始阶段这两个矩阵相乘为0，可以保证在初始阶段时，只有左边的主干生效。然后 BA 还会乘以一个缩放因子 a&#x2F;r， a 也由我们自己指定。</p>
<p>训练的时候，预训练的权重矩阵全部都是冻结的。</p>
<hr>
<h2 id="Leetcode-题"><a href="#Leetcode-题" class="headerlink" title="Leetcode 题"></a><strong>Leetcode 题</strong></h2>
        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> LLM_base</li>
        <li><strong>作者:</strong> 深情小小侯</li>
        <li><strong>创建于
                :</strong> 2024-06-20 11:38:11</li>
        
            <li>
                <strong>更新于
                    :</strong> 2024-10-19 11:01:20
            </li>
        
        <li>
            <strong>链接:</strong> https://redefine.ohevan.com/2024/06/20/LLM-base/
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            
            本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
            

        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/2024/06/22/LLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%8A%80%E6%9C%AF/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">LLM推理加速技术</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2024/06/15/bert%E5%9F%BA%E7%A1%80/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">bert基础</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container">
                <div class="comments-container pjax">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-swup-reload-script>
        import { init } from '/js/libs/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">LLM_base</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="nav-number">1.</span> <span class="nav-text">基本知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">2.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%98%AF"><span class="nav-number">2.1.</span> <span class="nav-text">self-attention 的时间复杂度是</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-%E5%9C%A8%E8%AE%A1%E7%AE%97%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E5%A6%82%E4%BD%95%E5%AF%B9padding%E4%BD%8D%E5%81%9Amask%EF%BC%9F"><span class="nav-number">2.2.</span> <span class="nav-text">self-attention 在计算的过程中，如何对padding位做mask？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81LayerNorm%E3%80%81BatchNorm%E3%80%81RMSNorm"><span class="nav-number">2.3.</span> <span class="nav-text">1、LayerNorm、BatchNorm、RMSNorm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E8%AF%B4%E4%B8%80%E4%B8%8B-Encoder-only%E3%80%81Encoder-Decoder%E3%80%81Decoder-only-%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.4.</span> <span class="nav-text">2、说一下 Encoder-only、Encoder-Decoder、Decoder-only 的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E5%88%86%E5%B8%83%E6%AF%94%E8%BE%83%E7%A8%80%E7%96%8F%EF%BC%8C%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">2.5.</span> <span class="nav-text">3.模型输出的分布比较稀疏，怎么处理？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-kl%E6%95%A3%E5%BA%A6%E7%9A%84%E5%85%AC%E5%BC%8F%E5%92%8Ckl%E6%95%A3%E5%BA%A6%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.6.</span> <span class="nav-text">4. kl散度的公式和kl散度与交叉熵的区别?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-%E6%96%87%E6%9C%ACembedding%E6%96%B9%E6%B3%95"><span class="nav-number">2.7.</span> <span class="nav-text">5. 介绍一下 文本embedding方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E8%AF%B4%E4%B8%80%E4%B8%8B-transformer%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%92%8C%E7%BB%86%E8%8A%82%EF%BC%8C%E5%A6%82%E4%BD%95%E5%B9%B6%E8%A1%8C"><span class="nav-number">2.8.</span> <span class="nav-text">6.说一下 transformer的模型架构和细节，如何并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%AF%B4%E4%B8%80%E4%B8%8B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%A0%A1%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F-p-tuning-v2%EF%BC%9F"><span class="nav-number">2.9.</span> <span class="nav-text">7.说一下大模型高校参数微调方式 p-tuning v2？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84LLM%E9%83%BD%E6%98%AFDecoder-only%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="nav-number">2.10.</span> <span class="nav-text">8 为什么现在的LLM都是Decoder only的架构？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">2.11.</span> <span class="nav-text">9.如何解决过拟合和欠拟合?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-L1%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">2.12.</span> <span class="nav-text">10. L1正则化和L2正则化的区别?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-%E5%B8%B8%E8%A7%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%EF%BC%88llama2%EF%BC%8Cglm%EF%BC%89%EF%BC%9F"><span class="nav-number">2.13.</span> <span class="nav-text">11.介绍一下 常见大模型结构（llama2，glm）？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-Sigmoid%E3%80%81ReLU-%E5%92%8C-GELU"><span class="nav-number">2.14.</span> <span class="nav-text">12 Sigmoid、ReLU 和 GELU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E9%87%8F%E5%8C%96%E5%8A%A0%E9%80%9F"><span class="nav-number">3.</span> <span class="nav-text">推理量化加速</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT"><span class="nav-number">4.</span> <span class="nav-text">GPT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-gpt%E6%BA%90%E7%A0%81past-key-value%E6%98%AF%E5%B9%B2%E5%95%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">1.  gpt源码past_key_value是干啥的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-pt-onebyone-%E6%AF%8F%E4%B8%80%E5%B1%82%E6%80%8E%E4%B9%88%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%EF%BC%9F"><span class="nav-number">4.2.</span> <span class="nav-text">2. pt onebyone 每一层怎么输入输出？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-chatgpt%E7%9A%84reward-model%E6%80%8E%E4%B9%88%E6%9D%A5%E7%9A%84%EF%BC%8C%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9F"><span class="nav-number">4.3.</span> <span class="nav-text">3 chatgpt的reward model怎么来的，三阶段？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert"><span class="nav-number">5.</span> <span class="nav-text">Bert</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E5%92%8C%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="nav-number">5.1.</span> <span class="nav-text">结构和组成部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%9C%A8BERT%E4%B8%AD%EF%BC%8Ctoken%E5%88%863%E7%A7%8D%E6%83%85%E5%86%B5%E5%81%9Amask%EF%BC%8C%E5%88%86%E5%88%AB%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">5.2.</span> <span class="nav-text">1.在BERT中，token分3种情况做mask，分别的作用是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-BERT%E8%AE%AD%E7%BB%83%E6%97%B6%E4%BD%BF%E7%94%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87-warm-up-%E7%AD%96%E7%95%A5%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%99%E4%B9%88%E5%81%9A%EF%BC%9F"><span class="nav-number">5.3.</span> <span class="nav-text">2.BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">5.4.</span> <span class="nav-text">3. BERT预训练任务</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG-%E7%9B%B8%E5%85%B3%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="nav-number">6.</span> <span class="nav-text">RAG 相关面试题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%81%8A%E4%B8%80%E4%B8%8BRAG%E9%A1%B9%E7%9B%AE%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="nav-number">6.1.</span> <span class="nav-text">1. 聊一下RAG项目总体思路？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BD%BF%E7%94%A8%E5%A4%96%E6%8C%82%E7%9F%A5%E8%AF%86%E5%BA%93%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%BA%E4%BA%86%E8%A7%A3%E5%86%B3%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-number">6.2.</span> <span class="nav-text">2. 使用外挂知识库主要是为了解决什么问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7RAG%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%95%88%E6%9E%9C%E5%A5%BD%E5%9D%8F%EF%BC%8C%E5%8D%B3%E6%8C%87%E6%A0%87%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">6.3.</span> <span class="nav-text">3. 如何评价RAG项目的效果好坏，即指标是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%9C%A8%E5%81%9ARAG%E9%A1%B9%E7%9B%AE%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E7%9A%84%EF%BC%9F"><span class="nav-number">6.4.</span> <span class="nav-text">4. 在做RAG项目过程中遇到哪些问题？怎么解决的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-RAG%E9%A1%B9%E7%9B%AE%E9%87%8C%E9%9D%A2%E6%9C%89%E5%93%AA%E4%B8%80%E4%BA%9B%E4%BA%AE%E7%82%B9%EF%BC%9F%E7%9B%AE%E5%89%8D%E5%BC%80%E6%BA%90%E7%9A%84RAG%E9%A1%B9%E7%9B%AE%E9%9D%9E%E5%B8%B8%E5%A4%9A%EF%BC%8C%E4%BD%A0%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%92%8C%E4%BB%96%E4%BB%AC%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">6.5.</span> <span class="nav-text">5. RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%80%8E%E4%B9%88%E6%9E%84%E5%BB%BA%E7%9A%84%EF%BC%8C%E4%BB%80%E4%B9%88%E8%A7%84%E6%A8%A1%EF%BC%8C%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%8C%87%E6%A0%87%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-number">6.6.</span> <span class="nav-text">6. 数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%BA%A7%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E8%BF%99%E4%BA%9B%E4%B8%8D%E5%90%8C%E5%BA%95%E5%BA%A7%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%80%E4%B9%88%E8%A7%84%E6%A8%A1%EF%BC%9F"><span class="nav-number">6.7.</span> <span class="nav-text">7. 模型底座是什么，这些不同底座什么区别，什么规模？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E4%BD%BF%E7%94%A8%E5%93%AA%E4%B8%80%E7%A7%8D%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%EF%BC%8C%E4%BB%80%E4%B9%88sft%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%96%B9%E6%B3%95%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%8C%E5%8E%9F%E7%90%86%E4%B8%8A%E8%A7%A3%E9%87%8A%E4%B8%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95%E7%9A%84%E5%B7%AE%E5%88%AB%EF%BC%9F"><span class="nav-number">6.8.</span> <span class="nav-text">8. 使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%98%AF%E6%80%8E%E4%B9%88%E5%81%9A%E7%9A%84%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89cot%EF%BC%8Ctot%E7%AD%89%E7%AD%89%EF%BC%8C%E8%BF%98%E6%98%AF%E5%8D%95%E8%BD%AE%EF%BC%9F"><span class="nav-number">6.9.</span> <span class="nav-text">9. 模型推理是怎么做的，有没有cot，tot等等，还是单轮？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E6%8E%A7%E6%80%A7%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%EF%BC%8C%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E5%8F%AF%E6%8E%A7%E6%80%A7%EF%BC%9F"><span class="nav-number">6.10.</span> <span class="nav-text">10. 大模型可控性如何实现，怎么保证可控性？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%9A%84%E5%B9%B3%E5%8F%B0%EF%BC%8C%E6%8E%A8%E7%90%86%E6%95%88%E7%8E%87%E6%80%8E%E4%B9%88%E6%A0%B7%EF%BC%8C%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E6%8E%A8%E7%90%86%E6%95%88%E7%8E%87%EF%BC%9F"><span class="nav-number">6.11.</span> <span class="nav-text">11. 模型部署的平台，推理效率怎么样，如何提升推理效率？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-RAG-%E8%A7%A3%E5%86%B3%E4%BA%86%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-number">6.12.</span> <span class="nav-text">.4 RAG 解决了哪些问题?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-%E7%BB%99%E4%B8%80%E4%B8%AA%E6%80%BB%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%A0%B7%E4%BE%8B%EF%BC%8C%E6%AF%8F%E4%B8%80%E6%AD%A5%E5%8C%85%E5%90%AB%E4%BB%80%E4%B9%88prompt%EF%BC%8C%E5%A4%9A%E8%BD%AE%E6%8E%A8%E7%90%86%E6%AF%8F%E4%B8%80%E6%AD%A5%E8%BE%93%E5%87%BA%E4%BB%80%E4%B9%88%E7%BB%93%E6%9E%9C%EF%BC%8C%E6%A8%A1%E6%8B%9F%E4%B8%80%E4%B8%8B%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F%E6%98%AF%E5%90%A6%E8%A6%81%E8%B0%83%E6%95%B4%E6%88%90%E8%BF%99%E6%A0%B7%EF%BC%8C%E6%95%B0%E6%8D%AE%E5%BD%A2%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E6%80%8E%E4%B9%88%E6%8B%86%E5%88%86%E6%88%90%E5%A4%9A%E8%BD%AE%E5%BD%A2%E5%BC%8F%EF%BC%9F"><span class="nav-number">6.13.</span> <span class="nav-text">13. 给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Agent-%E7%9B%B8%E5%85%B3%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="nav-number">7.</span> <span class="nav-text">Agent 相关面试题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LoRA"><span class="nav-number">8.</span> <span class="nav-text">LoRA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-lora%E7%9A%84%E5%8E%9F%E7%90%86"><span class="nav-number">8.1.</span> <span class="nav-text">1. lora的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-lora%E7%9A%84%E7%9F%A9%E9%98%B5%E6%80%8E%E4%B9%88%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%BA%E5%85%A80%EF%BC%9F"><span class="nav-number">8.2.</span> <span class="nav-text">2.lora的矩阵怎么初始化？为什么要初始化为全0？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Leetcode-%E9%A2%98"><span class="nav-number">9.</span> <span class="nav-text">Leetcode 题</span></a></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">深情小小侯</a>
        </div>
        
            <script data-swup-reload-script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">访问人数</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">总访问量</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.5.0</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>







<script src="/js/tools/imageViewer.js" type="module"></script>

<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>





    
<script src="/js/tools/codeBlock.js" type="module"></script>




    
<script src="/js/layouts/lazyload.js" type="module"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js" type="module"></script>






    
<script src="/js/libs/minimasonry.min.js"></script>

    
<script src="/js/plugins/masonry.js" type="module"></script>




<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
