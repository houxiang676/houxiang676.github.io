<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="深情小小侯">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2024/06/22/llm推理加速技术/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="一个常规的LLM推理过程通常分为两个阶段：prefill和decode。通常会使用KV cache技术加速推理。 预填充阶段（prefill）。在这个阶段中，我们把整段prompt喂给模型做forward计算。如果采用KV cache技术，在这个阶段中我们会把prompt过 𝑊𝑘,𝑊𝑣 后得到的 𝑋𝑘,𝑋𝑣 保存在cache_k和cache_v中。这样在对后面的token计算att">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM推理加速技术">
<meta property="og:url" content="http://example.com/2024/06/22/LLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%8A%80%E6%9C%AF/index.html">
<meta property="og:site_name" content="深情小小侯的博客">
<meta property="og:description" content="一个常规的LLM推理过程通常分为两个阶段：prefill和decode。通常会使用KV cache技术加速推理。 预填充阶段（prefill）。在这个阶段中，我们把整段prompt喂给模型做forward计算。如果采用KV cache技术，在这个阶段中我们会把prompt过 𝑊𝑘,𝑊𝑣 后得到的 𝑋𝑘,𝑋𝑣 保存在cache_k和cache_v中。这样在对后面的token计算att">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-06-22T00:54:14.000Z">
<meta property="article:modified_time" content="2024-07-23T03:37:02.842Z">
<meta property="article:author" content="深情小小侯">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            LLM推理加速技术 -
        
        深情小小侯的博客
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/assets/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    window.config = {"hostname":"example.com","root":"/","language":"zh-CN","path":"search.xml"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":true,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"走得快走得慢不重要，走下去就是胜利","subtitle":{"text":[],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.5.0","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"文档":{"path":"/archives","icon":"fa-regular fa-archive"},"相册":{"icon":"fa-solid fa-image","submenus":{"假日出行":"/masonry","小乖":"/gallery"}}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"纸上得来终觉浅，绝知此事要躬行","links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    window.data = {"masonry":true};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                深情小小侯的博客
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        文档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-solid fa-image"></i>
                                        
                                        相册&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/masonry">假日出行
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="/gallery">小乖
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                文档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-solid fa-image"></i>
                                
                                相册&nbsp;<i class="group-hover:rotate-180 transition-transform fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="drawer-navbar-item text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/masonry">假日出行</a>
                            </li>
                        
                            <li class="drawer-navbar-item text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/gallery">小乖</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular">LLM推理加速技术</h1>
            
            </div>
            
                    
        
        
            <div class="article-header flex flex-row gap-2 items-center">
                <div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
                    <img src="/images/redefine-avatar.svg">
                </div>
                <div class="info flex flex-col justify-between">
                    <div class="author flex items-center">
                        <span class="name text-default-text-color text-lg font-semibold">深情小小侯</span>
                        
                            <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv4</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-06-22 08:54:14</span>
        <span class="mobile">2024-06-22 08:54:14</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-07-23 11:37:02</span>
            <span class="mobile">2024-07-23 11:37:02</span>
            <span class="hover-info">更新</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <p>一个常规的LLM推理过程通常分为两个阶段：<strong>prefill和decode</strong>。通常会使用KV cache技术加速推理。</p>
<p><strong>预填充阶段（prefill）。</strong>在这个阶段中，我们<strong>把整段prompt喂给模型做forward计算</strong>。如果<strong>采用KV cache技术，在这个阶段中我们会把prompt过</strong> 𝑊𝑘,𝑊𝑣 <strong>后得到的</strong> 𝑋𝑘,𝑋𝑣 <strong>保存在cache_k和cache_v中</strong>。这样在对后面的token计算attention时，我们就不需要对前面的token重复计算 𝑋𝑘,𝑋𝑣 了，可以帮助我们节省推理时间。</p>
<p><strong>生成response的阶段</strong>decode。在这个阶段中，<strong>我们根据prompt的prefill结果，一个token一个token地生成response</strong>。<br>同样，如果采用了KV cache，则每走完一个decode过程，我们就把对应response token的KV值存入cache中，以便能加速计算。例如对于图中的t4，它与cache中t0~t3的KV值计算完attention后，就把自己的KV值也装进cache中。对t6也是同理。<br><strong>由于Decode阶段的是逐一生成token的，因此它不能像prefill阶段那样能做大段prompt的并行计算，所以在LLM推理过程中，Decode阶段的耗时一般是更大的</strong>。</p>
<p>从上述过程中，我们可以发现使用KV cache做推理时的一些特点：</p>
<ul>
<li><strong>随着prompt数量变多和序列变长，KV cache也变大，对gpu显存造成压力</strong></li>
<li><strong>由于输出的序列长度无法预先知道，所以我们很难提前为KV cache量身定制存储空间</strong></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>分布式（Distribution）</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>量化（Quantization）</td>
<td></td>
</tr>
<tr>
<td></td>
<td><strong>剪枝（Pruning）</strong></td>
<td></td>
</tr>
<tr>
<td></td>
<td>知识蒸馏</td>
<td></td>
</tr>
<tr>
<td></td>
<td>算子融合</td>
<td></td>
</tr>
</tbody></table>
<p><strong>模型压缩</strong>:</p>
<ul>
<li><p>网络剪枝：结构化非结构化剪枝</p>
</li>
<li><p>知识蒸馏</p>
</li>
<li><p><strong>模型量化</strong></p>
</li>
<li><p>低秩分解</p>
</li>
</ul>
<p><strong>大模型引擎调度</strong></p>
<ul>
<li>VLLM，TensorRT-LLM，Orca，llama.cpp</li>
</ul>
<h2 id="1-VLLM"><a href="#1-VLLM" class="headerlink" title="1. VLLM"></a>1. VLLM</h2><p>vLLM 主要特性：</p>
<ol>
<li>先进的服务吞吐量</li>
<li>通过 PagedAttention 对注意力 key 和 value 进行内存管理</li>
<li>对传入请求的批处理</li>
</ol>
<p>总结,减少显存、提高吞吐量。如何优化KV cache，节省显存，提高推理吞吐量，就成了LLM推理框架需要解决的重点问题</p>
<p><strong>实现原理：</strong></p>
<p>通过 PagedAttention 对注意力 key 和 value 进行内存管理，降低显存，提高推理速度</p>
<p>pagedAttention 实现原理</p>
<ol>
<li>内存中保存Key、value 值，允许内存空间不需要连续。不必浪费大量显存。方法：内存中序列的逻辑通过块表映射到物理内存中，保证其逻辑性。</li>
<li>内存中可以共享内存，使prompt 计算可以共享。</li>
</ol>
<p>通过 PagedAttention 对注意力 key 和 value 进行内存管理,PagedAttention 还有一个关键优势：高效的内存共享。例如，在<em>并行采样</em>中，同一个提示会生成多个输出序列。在这种情况下，输出序列之间可以共享提示的计算和内存。</p>
<hr>
<ul>
<li><strong>Parallel Sampling</strong>：我给模型发送一个请求，希望它对prompt做续写，并给出三种不同的回答。我们管这个场景叫parallel sampling。在这个场景中，我们可以将prompt复制3次后拼接成1个batch喂给模型，让它做推理。但我们也需注意到，这种方式会产生prompt部分KV cache的重复存储。</li>
<li><strong>Beam Search</strong>：束搜索，这是LLM常用的deocde策略之一，即在每个decode阶段，我不是只产生1个token，而是产生top k个token（这里k也被称为束宽）。top k个token必然对应着此刻的top k个序列。我把这top k个序列喂给模型，假设词表的大小为|V|，那么在下一时刻，我就要在k*|V|个候选者中再选出top k，以此类推。不难想象每一时刻我把top k序列喂给模型时，它们的前置token中有大量的KV cache是重复的。</li>
<li><strong>Shared prefix：</strong>在某些大模型中，所有请求可能都会共享一个前置信息（比如system message: “假设你是一个有帮助的AI助手….”），这些前置信息没有必要重复存储KV cache</li>
</ul>
<h3 id="为什么Cache只存储K、V，而不存储Q？"><a href="#为什么Cache只存储K、V，而不存储Q？" class="headerlink" title="为什么Cache只存储K、V，而不存储Q？"></a>为什么Cache只存储K、V，而不存储Q？</h3><p>在Transformer架构的注意力机制中，Q（Query）代表当前输入Token的查询信息，它需要与所有先前和当前的K（Key）进行匹配来决定注意力的分配。因为Q是专门针对当前正在处理的Token的，所以它每次都是独一无二的，即便在没有Cache的情况下，对于序列中的每个新Token，Q都会改变。<br>相反，K（Key）和V（Value）通常与序列中已经处理过的Token相关，它们可以被缓存起来供后续的Token查询使用。存储K和V使得模型可以重用之前Token的信息，而不必重新计算它们，这样可以大大节省计算资源。如果存储Q值，对于每个新的Token，我们仍然需要重新计算整个Q与所有K的匹配度，这不会减少计算量。因此，只缓存KV值而不缓存Q值，是为了优化计算效率。</p>
<h3 id="Parallel-Sampling"><a href="#Parallel-Sampling" class="headerlink" title="Parallel Sampling"></a>Parallel Sampling</h3><p><strong>（1）首先，Prefill阶段，vLLM拿到Sample A1和Sample A2，根据其中的文字内容，为其分配逻辑块和物理块。</strong></p>
<ul>
<li><strong>分配逻辑块：</strong>对于A1，vLLM为其分配逻辑块block0和block1；对于A2，vLLM为其分配逻辑块block0和block1。<strong>需要注意的是，A1的逻辑块和A2的逻辑块是独立的（尽管它们都叫block0和block1）</strong>，你可以将A1和A2视作操作系统中两个独立运行的进程。</li>
<li><strong>分配物理块</strong>：对于A1和A2，虽然逻辑块独立，但因为它们的文字完全相同，所以可以<strong>在物理内存上共享相同的空间</strong>。所以A1的逻辑块block0&#x2F;1分别指向物理块block7&#x2F;1；A2的逻辑块block0&#x2F;1分别指向物理块block7&#x2F;1。我们设每个物理块下映射的逻辑块数量为<code>ref count</code>，所以对物理块block7&#x2F;1来说，它们的ref count都为2。</li>
</ul>
<p><strong>（2）然后，进入decode阶段，A1和A2各自做推理，得到第一个token，分别为<code>fathers</code>和<code>mothers</code>。</strong></p>
<ul>
<li><strong>将生成的token装入逻辑块</strong>：对于A1和A2来说，将其生成的token装入各自的逻辑块block1。</li>
<li><strong>触发物理块copy-on-write机制</strong>：由于fathers&#x2F;mothers是两个完全不同的token，因此对物理块block1触发复制机制，即在物理内存上新开辟一块空间。此时物理块block1只和A2的逻辑块block1映射，将其ref count减去1；物理块block3只和A1的逻辑块block1映射，将其ref count设为1。</li>
</ul>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>在并行采样中，从相同的提示生成多个输出序列。在这种情况下，可以在输出序列之间共享提示的计算和内存。通过其块表，PagedAttention能够自然地实现内存共享。类似于进程共享物理页，PagedAttention中的不同序列可以通过将它们的逻辑块映射到相同的物理块来共享块。为确保安全共享，PagedAttention跟踪物理块的引用计数并实现 Copy-on-Write 机制。</p>
<p>通过PagedAttention的内存共享机制，极大地降低了复杂采样算法（如ParallelSampling和BeamSearch）的内存开销，使其内存使用量下降了高达55%。这项优化可以直接带来最多2.2倍的吞吐量提升，从而使得LLM服务中使用这些采样方法变得更加实用。</p>
<h2 id="2-LLama-cpp"><a href="#2-LLama-cpp" class="headerlink" title="2. LLama.cpp"></a>2. LLama.cpp</h2><p>Llama.cpp 是一个轻量级、高效的 C++ 实现，用于运行大型语言模型（如 Meta 的 LLaMA 系列）。它允许在相对较低的硬件资源下（例如消费级 CPU 和少量内存）运行和推理大型语言模型。Llama.cpp 特别注重内存和计算资源的优化，使得在没有 GPU 的情况下也能进行模型推理。</p>
<h2 id="3-Flash-attention"><a href="#3-Flash-attention" class="headerlink" title="3. Flash-attention"></a>3. Flash-attention</h2><h4 id="FlashAttention-的改进"><a href="#FlashAttention-的改进" class="headerlink" title="FlashAttention 的改进"></a>FlashAttention 的改进</h4><p>FlashAttention 通过以下步骤优化了注意力计算：</p>
<ol>
<li><strong>分块处理</strong>：<ul>
<li>将输入矩阵分成较小的块，以减少内存占用并提高计算效率。</li>
<li>每个块独立进行计算，利用现代 GPU 的并行计算能力。</li>
</ul>
</li>
<li><strong>逐块计算注意力权重和应用</strong>：<ul>
<li>在每个块内，计算注意力权重和应用注意力权重的操作都在局部进行，避免了大规模矩阵的全局计算。</li>
<li>每个块内的计算顺序被设计为避免内存瓶颈和数据传输的开销。</li>
</ul>
</li>
<li><strong>优化的内存访问</strong>：<ul>
<li>通过优化内存访问模式，使得计算过程中需要的数据能够高效地被加载到 GPU 的高速缓存中。</li>
<li>减少了内存访问延迟和数据传输开销。</li>
</ul>
</li>
</ol>
<h2 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h2><p>：将显存优化进行到底</p>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li><p>数据并行（data parallelism, DP）：假设有 𝑁 张卡，每张卡都保存一个模型，每一次迭代（iteration&#x2F;step）都将batch数据分割成 𝑁 个等大小的micro-batch，每张卡根据拿到的micro-batch数据独立计算梯度，然后调用AllReduce计算梯度均值，每张卡再独立进行参数更新。</p>
<p>加速的，不会减少所用内存。有通信开销</p>
</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型有三层：L0, L1, L2</span><br><span class="line"># 每层有两个神经元</span><br><span class="line"># 两张卡</span><br><span class="line"></span><br><span class="line">GPU0: </span><br><span class="line">L0 | L1 | L2</span><br><span class="line">---|----|---</span><br><span class="line">a0 | b0 | c0</span><br><span class="line">a1 | b1 | c1</span><br><span class="line"></span><br><span class="line">GPU1:</span><br><span class="line">L0 | L1 | L2</span><br><span class="line">---|----|---</span><br><span class="line">a0 | b0 | c0</span><br><span class="line">a1 | b1 | c1</span><br></pre></td></tr></table></figure></div>

<ul>
<li>模型并行（model parallelism&#x2F;tensor parallelism, MP&#x2F;TP）：有的tensor&#x2F;layer很大，一张卡放不下，将tensor分割成多块，一张卡存一块。</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型有三层：L0, L1, L2</span><br><span class="line"># 每层有两个神经元</span><br><span class="line"># 两张卡</span><br><span class="line"></span><br><span class="line">GPU0:</span><br><span class="line">L0 | L1 | L2</span><br><span class="line">---|----|---</span><br><span class="line">a0 | b0 | c0</span><br><span class="line"></span><br><span class="line">GPU1:</span><br><span class="line">L0 | L1 | L2</span><br><span class="line">---|----|---</span><br><span class="line">a1 | b1 | c1</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>流水并行（pipeline parallelism, PP）：将网络按层切分，划分成多组，一张卡存一组。</p>
<p>Pipeline并行的支持是通过PyTorch中的torch.distributed.pipeline.sync.Pipe类实现的。然而，这个类有两个主要限制：(1) 仅在模型实现为torch.nn.Sequential模块时有效，(2) 它要求每个模块的输入和输出必须是单个张量或张量元组。</p>
</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型有8层</span><br><span class="line"># 两张卡</span><br><span class="line"></span><br><span class="line">======================  =====================</span><br><span class="line">|  L0 | L1 | L2 | L3 |  | L4 | L5 | L6 | L7 |</span><br><span class="line">======================  =====================</span><br><span class="line">        GPU0                 GPU1</span><br><span class="line"></span><br><span class="line"># 设想一下，当GPU0在进行（前向/后向）计算时，GPU1在干嘛？闲着</span><br><span class="line"># 当GPU1在进行（前向/后向)计算时，GPU0在干嘛？闲着</span><br><span class="line"># 为了防止”一卡工作，众卡围观“，实践中PP也会把batch数据分割成</span><br><span class="line"># 多个micro-batch，流水线执行</span><br></pre></td></tr></table></figure></div>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> LLM推理加速技术</li>
        <li><strong>作者:</strong> 深情小小侯</li>
        <li><strong>创建于
                :</strong> 2024-06-22 08:54:14</li>
        
            <li>
                <strong>更新于
                    :</strong> 2024-07-23 11:37:02
            </li>
        
        <li>
            <strong>链接:</strong> https://redefine.ohevan.com/2024/06/22/LLM推理加速技术/
        </li>
        <li>
            <strong>
                版权声明:
            </strong>
            
            本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
            

        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/2024/06/22/LLama-%E6%9E%B6%E6%9E%84/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">LLama_架构</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2024/06/20/LLM-base/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">LLM_base</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container">
                <div class="comments-container pjax">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-swup-reload-script>
        import { init } from '/js/libs/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">LLM推理加速技术</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-VLLM"><span class="nav-number">1.</span> <span class="nav-text">1. VLLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Cache%E5%8F%AA%E5%AD%98%E5%82%A8K%E3%80%81V%EF%BC%8C%E8%80%8C%E4%B8%8D%E5%AD%98%E5%82%A8Q%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">为什么Cache只存储K、V，而不存储Q？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallel-Sampling"><span class="nav-number">1.2.</span> <span class="nav-text">Parallel Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beam-Search"><span class="nav-number">1.3.</span> <span class="nav-text">Beam Search</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-LLama-cpp"><span class="nav-number">2.</span> <span class="nav-text">2. LLama.cpp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Flash-attention"><span class="nav-number">3.</span> <span class="nav-text">3. Flash-attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSpeed"><span class="nav-number">4.</span> <span class="nav-text">DeepSpeed</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">4.1.</span> <span class="nav-text">背景</span></a></li></ol></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">深情小小侯</a>
        </div>
        
            <script data-swup-reload-script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">访问人数</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">总访问量</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span>
            <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.5.0</a></span>
        </div>
        
        
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>







<script src="/js/tools/imageViewer.js" type="module"></script>

<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>





    
<script src="/js/tools/codeBlock.js" type="module"></script>




    
<script src="/js/layouts/lazyload.js" type="module"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js" type="module"></script>






    
<script src="/js/libs/minimasonry.min.js"></script>

    
<script src="/js/plugins/masonry.js" type="module"></script>




<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
